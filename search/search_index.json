{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Principles","text":""},{"location":"#data-science-project-template","title":"Data Science Project Template","text":"<p>Keeping well organized data science projects is difficult, but important. Ideally every project should be reproducible\u2014that is, anyone starting from the same data, code, and similar hardware should be able to obtain the same  results. </p> <p>The goal of this template is to make our data science work at Talus Bio reproducible and approachable. Using this template ensures that we organize code and data consistently across projects. This means when you join another project that is organized using this template, you will immediately know the lay of the land. </p> <p>This template uses cookiecutter to create a directory structure for a repository that is consistent and well-suited for data science projects. There are biases and assumptions in this template: we assume that Python will be your primary tool (although it doesn\u2019t have to be) and that some amount of modeling will be performed as part of the  project (although there doesn\u2019t have to be). As such, this template is intended to serve as a guideline rather than a rule and you should feel free to modify it as needed for your specific project.</p> <p>This template was inspired by the The Cookiecutter Data Science Project and Bill Noble\u2019s \u201cA Quick Guide to Organizing Computational Biology Projects\u201d.</p>"},{"location":"#principles-guiding-this-template","title":"Principles Guiding This Template","text":"<p>Although this template is flexible, these principles guide its design:</p>"},{"location":"#data-analysis-is-non-linear","title":"Data Analysis is Non-Linear","text":"<p>We try many things that work and even more that don\u2019t. Additionally, we often have long running tasks that don\u2019t need to run again unless their dependencies change. Thus, we can think of the tasks in a data analysis as a directed acyclic graph (DAG), where each task has some defined set of dependencies and an output which other tasks may depend on.</p> <p>So how can we make the final analysis reproducible? We like to use GNU <code>make</code>.  The <code>make</code> utility is common on MacOS and Linux systems and describes the  dependency relationships between files. Given a <code>Makefile</code> (the file that  defines these relationships), we can verify that our analysis completes the same way from start to finish, every time.1</p> <p>We recommend editing the <code>run</code> rule in your <code>Makefile</code> to run your analyses and create your final results in the <code>results</code> subdirectory. When this recommendation is followed, an analysis can be completely regenerated with:</p> <pre><code>$ make run\n</code></pre>"},{"location":"#data-is-immutable","title":"Data is Immutable","text":"<p>The original data, whatever its form, is the starting point for all analyses and should never be modified. Furthermore, transformations and modifications that need to me made to the data should never be made manually and should never overwrite the original data\u2014such practices inevitability leads to irreproducible analyses.</p> <p>Instead, all alterations should be made programmatically and these intermediate versions of the data can be saved alongside the original, if needed. Saving intermediate files is most useful for transformations that take a long time to perform. Otherwise, creating a reusable function to achieve the desired transformation is typically more desirable.</p> <p>All data should be stored in the <code>data</code> subdirectory, and it should not be version controlled (don\u2019t add it to your git repository). Git should only be used to track only files that are hand-edited\u2014data should not be\u2014and it is often too large to work with in git effectively anyway. Additionally, even when the data is small enough to feasibly manage in git, storing it in the repository creates another point of potential leakage for data we need to keep private: We don\u2019t want private data leaking because we accidently set a repository to public!</p> <p>When possible, data science projects should include a <code>sync</code> rule in their <code>Makefile</code>. This rule should download the data needed to run all of the analyses that are part of the project. We have this rule configured to pull data from specified subdirectories of a specified Amazon Web Services (AWS) S3 bucket by default. However, this rule can be configured to pull data from anywhere that your data is stored.2 For any project following these guidelines, you can synchronize your data directory with:</p> <pre><code>$ make sync\n</code></pre>"},{"location":"#notebooks-are-for-exploration-and-communication","title":"Notebooks are for Exploration and Communication","text":"<p>Notebook packages like the Jupyter notebook and other literate programming tools are very effective for exploratory data analysis. However, these tools can be less effective for reproducing an analysis. Since notebooks are challenging objects for version control (e.g., diffs of the json are often not human-readable and merging is near impossible), we recommended not collaborating directly with others on the same Jupyter notebook. We recommend these steps for using notebooks effectively:</p> <ul> <li> <p>Jupyter notebooks are stored in the <code>notebooks</code> subdirectory. Typically we   store notebooks in their final state, which makes them easy to share with   team members.</p> </li> <li> <p>We like to use additional subdirectories to divide different types of analyses,   particularly for projects that take place over longer periods of time. We   recommend the format <code>&lt;date&gt;_&lt;description&gt;</code> for these directories (e.g.,   <code>2021-07-08_exporatory-analyses</code>). If multiple people are writing notebooks   in the same repository, we recommend an additional subdirectory   specifying the GitHub username of the notebook authors. For instance, I may   have a notebook here:   <pre><code>notebooks/wfondrie/2021-07-08_exploratory-analyses/1_visualizations.ipynb\n</code></pre></p> </li> <li> <p>Follow a naming convention that shows the order in which the analysis was   performed. We recommend the format <code>&lt;step&gt;_&lt;description&gt;.ipynb</code> (e.g.,   <code>0.3_visualize-distributions.ipynb</code>).</p> </li> <li> <p>Refactor the good parts. Don\u2019t write code to do the same task in multiple   notebooks. If it\u2019s a data preprocessing task, put it in the pipeline at   <code>src/make_dataset.py</code> and load data from data/interim. If it\u2019s useful   utility code, refactor it to <code>src</code>. Also, don\u2019t be afraid to write useful   scripts alongside your notebooks. Finally, if you find yourself writing the   same code for multiple projects, consider writing a Python Package. See my   cookiecutter   template for a good   starting point.</p> </li> <li> <p>Jupyter notebooks are designed for literate programming, so take advantage!   Create markdown cells that describe the experiments contained within the   notebook and the different stages of analysis, as well as key results.</p> </li> <li> <p>Be consistent with how you save results. I store all of my figures in a   <code>figures</code> subdirectory alongside my notebooks. Additionally, these should not   be version controlled, unless there is a specific reason to do so.</p> </li> </ul>"},{"location":"#reproducibility-starts-with-the-environment","title":"Reproducibility Starts With the Environment","text":"<p>Our environment defines the tools, including their versions, that we need to complete our analyses. To manage your environment, we recommend using the conda package manager. Why conda? Although largely thought of as a Python package manager, conda can download and install a wide variety of useful software in a reproducible manner. For example, one of the proteomics tools we routinely use is a Java program, EncyclopeDIA, and available through the bioconda channel.</p> <p>Once we know what software we need, we can complete the <code>environment.yaml</code> file, which will allow anyone to install the same software and reproduce our analysis. When ready, create your environment using this file with:</p> <pre><code>$ make env\n</code></pre> <ol> <li> <p>As your analyses grow, it may be beneficial to look at other tools, such Snakemake, to orchestrate your analyses.\u00a0\u21a9</p> </li> <li> <p>If you\u2019re using public mass spectrometry data from MassIVE or PRIDE, consider using ppx to download the data reproducibly.\u00a0\u21a9</p> </li> </ol>"},{"location":"tips/","title":"Tips and Tricks","text":""},{"location":"tips/#setting-up-your-conda-environment","title":"Setting up Your Conda Environment","text":"<p>First make sure that your <code>environment.yaml</code> contains the tools that you need for your analysis. The <code>Makefile</code> included in the repository already contains the rules to create or update the environment for your project, based on the <code>environment.yaml</code>. From the root of your project, run:</p> <pre><code>$ make env\n</code></pre> <p>This will install your conda environment directly into the root of the repository. Follow the instructions to activate your new environment.</p> <p>Tip: Your shell prompt now likely shows the full path to your conda environment, which may be unwieldy. Update your conda config to hide this with:</p> <pre><code>conda config --set env_prompt '({name})'\n</code></pre> <p>If you discover that you need an additional tool, all you need to do is add it to your <code>environment.yaml</code> and run <code>make env</code> again.</p>"},{"location":"tips/#the-src-directory-works-like-a-python-package","title":"The <code>src</code> Directory Works Like a Python Package!","text":"<p>The default files we include allow the <code>src</code> directory to work like a Python package. If you\u2019ve installed and activated your environment above, then its ready to go! You can now use the functions and classes defined in <code>src</code> within your Jupyter notebooks:</p> <pre><code># OPTIONAL: Load the \"autoreload\" extension so that code can change\n%load_ext autoreload\n\n# OPTIONAL: always reload modules so that as you change code in src, it gets loaded\n%autoreload 2\n\n# Import your functions and classes from 'src'\nfrom src.data import make_dataset\n</code></pre>"},{"location":"tips/#enable-python-code-formatting-with-black","title":"Enable Python Code Formatting With Black","text":"<p>Black is a Python code formatting tool that helps us maintain uniform code formats throughout our projects. The easiest way to use Black is to set it up as a pre-commit hook. This way Black will run whenever you commit changes to your repository.</p> <p>To enable Black, run from the root of your project:</p> <pre><code>$ pre-commit install\n</code></pre>"},{"location":"usage/","title":"How to Use This Template","text":""},{"location":"usage/#1-install-dependencies","title":"1. Install Dependencies","text":"<p>To use this template, you\u2019ll need to install these dependencies:</p> <ul> <li>Python 3.9+ - We recommend through    Miniconda.</li> <li>Cookiecutter &gt;=    1.4.0</li> </ul> <p>Cookiecutter can be installed with pip by or conda depending on how you manage  your Python packages:</p> <pre><code>$ pip install cookiecutter\n</code></pre> <p>or</p> <pre><code>$ conda config --add channels conda-forge\n$ conda install cookiecutter\n</code></pre>"},{"location":"usage/#2-start-a-new-project","title":"2. Start a New Project","text":"<p>To start a new data science project first run:</p> <pre><code>$ cookiecutter gh:TalusBio/cookiecutter-data-science\n</code></pre> <p>After running the command you will be directed through a series of prompts to finish setting up your project. The directory structure of your new project will look like this: <pre><code>\u251c\u2500\u2500 Makefile                       &lt;- Makefile with commands like `make data` or `make env`\n\u251c\u2500\u2500 README.md                      &lt;- The top-level README for developers using this project.\n\u251c\u2500\u2500 data                           &lt;- The original, immutable data.\n\u251c\u2500\u2500 docs                           &lt;- Manuscript drafts, presentations, etc.\n\u251c\u2500\u2500 notebooks                      &lt;- Jupyter notebooks and/or analysis scripts. \n\u2502   \u2514\u2500\u2500 wfondrie                   &lt;- Create a subdirectory with your username.\n\u2502       \u2514\u2500\u2500 2022-11-01_my-analysis &lt;- Create a dated subdirectory for each analysis.\n\u2502           \u251c\u2500\u2500 notebook.ipynb     &lt;- The analysis notebook or script.\n\u2502           \u2514\u2500\u2500 figures            &lt;- A subdirectory to put the generated figures\n\u251c\u2500\u2500 environment.yml                &lt;- Specifies the dependencies to build a conda environment.\n\u2502                                     Create the environment with `make env &amp;&amp; conda activate ./envs`\n\u251c\u2500\u2500 pyproject.toml                 &lt;- Specifies Python configuration for our local Python package.\n\u251c\u2500\u2500 src                            &lt;- Source code for the local Python package to use in this project.\n\u2502   \u2514\u2500\u2500 __init__.py                &lt;- Makes src a Python module\n\u2514\u2500\u2500 .env                           &lt;- Define sensitive environment variables. This is intentionally \n                                      ignored by git by default. Do not commit this file!\n</code></pre></p> <p>Feel free to modify these directories and files as best fits your needs. For example, if your project does not use Python, you may want to remove a couple files: <code>pyproject.toml</code> and <code>src/__init__.py</code>.</p>"},{"location":"usage/#3-customize-and-do-cool-things","title":"3. Customize and Do Cool Things","text":"<p>Your first task in your new project should be to edit <code>environment.yaml</code>, so that it contains all of the software dependencies for your project. Once it  is ready, create your project environment with:</p> <pre><code>$ make env\n</code></pre> <p>Then follow the instructions to activate your new environment. With your environment prepared and activated, you\u2019re ready to start your analyses!</p>"}]}